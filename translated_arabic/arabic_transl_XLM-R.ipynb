{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Fine-tuning & Testing XLM-Roberta on Arabic-translated & Arabic Data </h1>\n",
    "Carley Reardon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English-to-Arabic Train Tweets: (21124, 6)\n",
      "   Unnamed: 0                  id sentiment  \\\n",
      "0           0  264183816548130816  positive   \n",
      "1           1  263405084770172928  negative   \n",
      "2           2  262163168678248449  negative   \n",
      "3           3  264249301910310912  negative   \n",
      "4           4  262682041215234048   neutral   \n",
      "\n",
      "                                            original  \\\n",
      "0  Gas by my house hit $3.39!!!! I\\u2019m going t...   \n",
      "1  Theo Walcott is still shit\\u002c watch Rafa an...   \n",
      "2  its not that I\\u2019m a GSP fan\\u002c i just h...   \n",
      "3  Iranian general says Israel\\u2019s Iron Dome c...   \n",
      "4  Tehran\\u002c Mon Amour: Obama Tried to Establi...   \n",
      "\n",
      "                                        preprocessed  \\\n",
      "0  gas house hit $ 3.39 i\\u2019 m going chapel hi...   \n",
      "1  theo walcott shit\\u002c watch rafa johnny deal...   \n",
      "2  i\\u2019 m gsp fan\\u002c hate nick diaz can\\u20...   \n",
      "3  iranian general says israel\\u2019s iron dome c...   \n",
      "4  tehran\\u002c mon amour obama tried establish t...   \n",
      "\n",
      "                                          translated  \n",
      "0                 \"منزل غاز ضرب بـ 3.39 مليون دولار\"  \n",
      "1                   مشاهدة rafa Jonny صفقة يوم السبت  \n",
      "2  (ط) / u2019 م(م) gsp مُعجب / u________________...  \n",
      "3  الجنرال الإيراني العام يقول أن israel\\U2019s r...  \n",
      "4  Tranran\\u0002c mona amamama transby struction ...  \n",
      "\n",
      "English-to-Arabic Test Tweets: (28422, 6)\n",
      "   Unnamed: 0            id sentiment  \\\n",
      "0           0  2.642380e+17  positive   \n",
      "1           1  2.187750e+17  positive   \n",
      "2           2  2.589650e+17   neutral   \n",
      "3           3  2.629260e+17  negative   \n",
      "4           4  1.718740e+17   neutral   \n",
      "\n",
      "                                            original  \\\n",
      "0  @jjuueellzz down in the Atlantic city, ventnor...   \n",
      "1  Musical awareness: Great Big Beautiful Tomorro...   \n",
      "2  On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...   \n",
      "3  Kapan sih lo ngebuktiin,jan ngomong doang Susa...   \n",
      "4  Excuse the connectivity of this live stream, f...   \n",
      "\n",
      "                                        preprocessed  \\\n",
      "0  atlantic city ventnor margate ocean city area ...   \n",
      "1  musical awareness great big beautiful tomorrow...   \n",
      "2  radio786 100.4fm 7:10 fri oct 19 labour analys...   \n",
      "3  kapan sih lo ngebuktiin jan ngomong doang susa...   \n",
      "4  excuse connectivity live stream baba amr activ...   \n",
      "\n",
      "                                          translated  \n",
      "0  منطقة مدينة المحيط الأطلسي في منطقة مدينة المح...  \n",
      "1  \"معرفة موسيقية عظيمة جداً جداً وجميلاً جداً غد...  \n",
      "2                               مُحللة عمالية مُحللة  \n",
      "3  Kaban sih lo ngebuktiin Jan ngomong Doang susa...  \n",
      "4  مُوَدَمُ مُقَرَّرُ مباشرُ مُوَدّمُ مُقَرَّرُ م...  \n",
      "\n",
      "Arabic Test Tweets: (3353, 4)\n",
      "                   id sentiment  \\\n",
      "0  783555835494592513  positive   \n",
      "1  783582397166125056  positive   \n",
      "2  783592390728769536  positive   \n",
      "3  783597390070685696  positive   \n",
      "4  783617442031472640   neutral   \n",
      "\n",
      "                                            original  \\\n",
      "0  إجبار أبل على التعاون على فك شفرة اجهزتها http...   \n",
      "1  RT @20fourMedia: #غوغل تتحدى أبل وأمازون بأجهز...   \n",
      "2  جوجل تنافس أبل وسامسونج بهاتف جديد https://t.c...   \n",
      "3  رئيس شركة أبل: الواقع المعزز سيصبح أهم من الطع...   \n",
      "4  ساعة أبل في الأسواق مرة أخرى https://t.co/dY2x...   \n",
      "\n",
      "                                      preprocessed  \n",
      "0        إجبار أبل على التعاون على فك شفرة اجهزتها  \n",
      "1              غوغل تتحدى أبل وأمازون بأجهزة جديدة  \n",
      "2               جوجل تنافس أبل وسامسونج بهاتف جديد  \n",
      "3  رئيس شركة أبل الواقع المعزز سيصبح أهم من الطعام  \n",
      "4                     ساعة أبل في الأسواق مرة أخرى  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# first import our translated and original data\n",
    "a_prep_path = \"/projectnb/cs505/reardonc/CS505-Spring-MultiLingual-Twitter-Classification/data/preprocessed/arabic/\"\n",
    "e_transl_path = \"/projectnb/cs505/reardonc/CS505-Spring-MultiLingual-Twitter-Classification/data/translated/english/\"\n",
    "\n",
    "e_train_df = pd.read_csv(e_transl_path+\"english_train_tweets.csv\")\n",
    "e_test_df = pd.read_csv(e_transl_path+\"english_test_tweets.csv\")\n",
    "a_test_df = pd.read_csv(a_prep_path+\"arabic_test_tweets.csv\")\n",
    "\n",
    "print(\"English-to-Arabic Train Tweets: \"+str(e_train_df.shape))\n",
    "print(e_train_df.head(5))\n",
    "print(\"\")\n",
    "print(\"English-to-Arabic Test Tweets: \"+str(e_test_df.shape))\n",
    "print(e_test_df.head(5))\n",
    "print(\"\")\n",
    "print(\"Arabic Test Tweets: \"+str(a_test_df.shape))\n",
    "print(a_test_df.head(5))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (4.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: sacremoses in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: packaging in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (2020.5.14)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: importlib-metadata in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (1.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (4.46.0)\n",
      "Requirement already satisfied: filelock in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: joblib in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: click in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (0.1.95)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "# set up data for model\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=True)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "\n",
    "for idx, row in e_train_df.iterrows():\n",
    "    #print(row)\n",
    "    tw = str(row['translated'])\n",
    "    if row['sentiment'] == \"positive\":\n",
    "        label = 1\n",
    "    elif row['sentiment'] == \"negative\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 2\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tw,                      \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 128,\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )   \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    labels.append(label)\n",
    "\n",
    "for idx, row in e_test_df.iterrows():\n",
    "    tw = str(row['translated'])\n",
    "    if row['sentiment'] == \"positive\":\n",
    "        label = 1\n",
    "    elif row['sentiment'] == \"negative\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 2\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tw,                      \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 128,\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )   \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    labels.append(label)\n",
    "    \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44,591 training samples\n",
      "4,955 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "batch_size = 16\n",
    " \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359afad5e5534a56ad73be3ff2e440bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=512.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa593a52ca8e4edbad6c52d26f9237e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "# Load XLM-R\n",
    "xlm_model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\", # Use the base XLM-R model, with an uncased vocab.\n",
    "    num_labels = 3, # The number of output labels--3 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "xlm_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "optimizer_xlm = AdamW(xlm_model.parameters(),\n",
    "                  lr = 5e-6,\n",
    "                  eps = 1e-8\n",
    "                )\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer_xlm, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def training_helper(model, train_dataloader, validation_dataloader, optimizer, scheduler, \n",
    "                    seed_val=42, epochs=3, ):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    training_stats = []\n",
    "\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            result = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels,\n",
    "                        return_dict=True)\n",
    "\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            with torch.no_grad():        \n",
    "                result = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels,\n",
    "                            return_dict=True)\n",
    "\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  2,787.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  2,787.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  2,787.    Elapsed: 0:00:17.\n",
      "  Batch   160  of  2,787.    Elapsed: 0:00:23.\n",
      "  Batch   200  of  2,787.    Elapsed: 0:00:29.\n",
      "  Batch   240  of  2,787.    Elapsed: 0:00:35.\n",
      "  Batch   280  of  2,787.    Elapsed: 0:00:41.\n",
      "  Batch   320  of  2,787.    Elapsed: 0:00:46.\n",
      "  Batch   360  of  2,787.    Elapsed: 0:00:52.\n",
      "  Batch   400  of  2,787.    Elapsed: 0:00:58.\n",
      "  Batch   440  of  2,787.    Elapsed: 0:01:04.\n",
      "  Batch   480  of  2,787.    Elapsed: 0:01:10.\n",
      "  Batch   520  of  2,787.    Elapsed: 0:01:16.\n",
      "  Batch   560  of  2,787.    Elapsed: 0:01:21.\n",
      "  Batch   600  of  2,787.    Elapsed: 0:01:27.\n",
      "  Batch   640  of  2,787.    Elapsed: 0:01:33.\n",
      "  Batch   680  of  2,787.    Elapsed: 0:01:39.\n",
      "  Batch   720  of  2,787.    Elapsed: 0:01:45.\n",
      "  Batch   760  of  2,787.    Elapsed: 0:01:51.\n",
      "  Batch   800  of  2,787.    Elapsed: 0:01:56.\n",
      "  Batch   840  of  2,787.    Elapsed: 0:02:02.\n",
      "  Batch   880  of  2,787.    Elapsed: 0:02:08.\n",
      "  Batch   920  of  2,787.    Elapsed: 0:02:14.\n",
      "  Batch   960  of  2,787.    Elapsed: 0:02:20.\n",
      "  Batch 1,000  of  2,787.    Elapsed: 0:02:25.\n",
      "  Batch 1,040  of  2,787.    Elapsed: 0:02:31.\n",
      "  Batch 1,080  of  2,787.    Elapsed: 0:02:37.\n",
      "  Batch 1,120  of  2,787.    Elapsed: 0:02:43.\n",
      "  Batch 1,160  of  2,787.    Elapsed: 0:02:49.\n",
      "  Batch 1,200  of  2,787.    Elapsed: 0:02:55.\n",
      "  Batch 1,240  of  2,787.    Elapsed: 0:03:00.\n",
      "  Batch 1,280  of  2,787.    Elapsed: 0:03:06.\n",
      "  Batch 1,320  of  2,787.    Elapsed: 0:03:12.\n",
      "  Batch 1,360  of  2,787.    Elapsed: 0:03:18.\n",
      "  Batch 1,400  of  2,787.    Elapsed: 0:03:24.\n",
      "  Batch 1,440  of  2,787.    Elapsed: 0:03:29.\n",
      "  Batch 1,480  of  2,787.    Elapsed: 0:03:35.\n",
      "  Batch 1,520  of  2,787.    Elapsed: 0:03:41.\n",
      "  Batch 1,560  of  2,787.    Elapsed: 0:03:47.\n",
      "  Batch 1,600  of  2,787.    Elapsed: 0:03:53.\n",
      "  Batch 1,640  of  2,787.    Elapsed: 0:03:59.\n",
      "  Batch 1,680  of  2,787.    Elapsed: 0:04:04.\n",
      "  Batch 1,720  of  2,787.    Elapsed: 0:04:10.\n",
      "  Batch 1,760  of  2,787.    Elapsed: 0:04:16.\n",
      "  Batch 1,800  of  2,787.    Elapsed: 0:04:22.\n",
      "  Batch 1,840  of  2,787.    Elapsed: 0:04:28.\n",
      "  Batch 1,880  of  2,787.    Elapsed: 0:04:33.\n",
      "  Batch 1,920  of  2,787.    Elapsed: 0:04:39.\n",
      "  Batch 1,960  of  2,787.    Elapsed: 0:04:45.\n",
      "  Batch 2,000  of  2,787.    Elapsed: 0:04:51.\n",
      "  Batch 2,040  of  2,787.    Elapsed: 0:04:57.\n",
      "  Batch 2,080  of  2,787.    Elapsed: 0:05:02.\n",
      "  Batch 2,120  of  2,787.    Elapsed: 0:05:08.\n",
      "  Batch 2,160  of  2,787.    Elapsed: 0:05:14.\n",
      "  Batch 2,200  of  2,787.    Elapsed: 0:05:20.\n",
      "  Batch 2,240  of  2,787.    Elapsed: 0:05:26.\n",
      "  Batch 2,280  of  2,787.    Elapsed: 0:05:31.\n",
      "  Batch 2,320  of  2,787.    Elapsed: 0:05:37.\n",
      "  Batch 2,360  of  2,787.    Elapsed: 0:05:43.\n",
      "  Batch 2,400  of  2,787.    Elapsed: 0:05:49.\n",
      "  Batch 2,440  of  2,787.    Elapsed: 0:05:55.\n",
      "  Batch 2,480  of  2,787.    Elapsed: 0:06:00.\n",
      "  Batch 2,520  of  2,787.    Elapsed: 0:06:06.\n",
      "  Batch 2,560  of  2,787.    Elapsed: 0:06:12.\n",
      "  Batch 2,600  of  2,787.    Elapsed: 0:06:18.\n",
      "  Batch 2,640  of  2,787.    Elapsed: 0:06:24.\n",
      "  Batch 2,680  of  2,787.    Elapsed: 0:06:29.\n",
      "  Batch 2,720  of  2,787.    Elapsed: 0:06:35.\n",
      "  Batch 2,760  of  2,787.    Elapsed: 0:06:41.\n",
      "\n",
      "  Average training loss: 0.97\n",
      "  Training epcoh took: 0:06:45\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.55\n",
      "  Validation Loss: 0.92\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  2,787.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  2,787.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  2,787.    Elapsed: 0:00:17.\n",
      "  Batch   160  of  2,787.    Elapsed: 0:00:23.\n",
      "  Batch   200  of  2,787.    Elapsed: 0:00:29.\n",
      "  Batch   240  of  2,787.    Elapsed: 0:00:35.\n",
      "  Batch   280  of  2,787.    Elapsed: 0:00:41.\n",
      "  Batch   320  of  2,787.    Elapsed: 0:00:46.\n",
      "  Batch   360  of  2,787.    Elapsed: 0:00:52.\n",
      "  Batch   400  of  2,787.    Elapsed: 0:00:58.\n",
      "  Batch   440  of  2,787.    Elapsed: 0:01:04.\n",
      "  Batch   480  of  2,787.    Elapsed: 0:01:10.\n",
      "  Batch   520  of  2,787.    Elapsed: 0:01:15.\n",
      "  Batch   560  of  2,787.    Elapsed: 0:01:21.\n",
      "  Batch   600  of  2,787.    Elapsed: 0:01:27.\n",
      "  Batch   640  of  2,787.    Elapsed: 0:01:33.\n",
      "  Batch   680  of  2,787.    Elapsed: 0:01:39.\n",
      "  Batch   720  of  2,787.    Elapsed: 0:01:44.\n",
      "  Batch   760  of  2,787.    Elapsed: 0:01:50.\n",
      "  Batch   800  of  2,787.    Elapsed: 0:01:56.\n",
      "  Batch   840  of  2,787.    Elapsed: 0:02:02.\n",
      "  Batch   880  of  2,787.    Elapsed: 0:02:08.\n",
      "  Batch   920  of  2,787.    Elapsed: 0:02:13.\n",
      "  Batch   960  of  2,787.    Elapsed: 0:02:19.\n",
      "  Batch 1,000  of  2,787.    Elapsed: 0:02:25.\n",
      "  Batch 1,040  of  2,787.    Elapsed: 0:02:31.\n",
      "  Batch 1,080  of  2,787.    Elapsed: 0:02:37.\n",
      "  Batch 1,120  of  2,787.    Elapsed: 0:02:42.\n",
      "  Batch 1,160  of  2,787.    Elapsed: 0:02:48.\n",
      "  Batch 1,200  of  2,787.    Elapsed: 0:02:54.\n",
      "  Batch 1,240  of  2,787.    Elapsed: 0:03:00.\n",
      "  Batch 1,280  of  2,787.    Elapsed: 0:03:06.\n",
      "  Batch 1,320  of  2,787.    Elapsed: 0:03:11.\n",
      "  Batch 1,360  of  2,787.    Elapsed: 0:03:17.\n",
      "  Batch 1,400  of  2,787.    Elapsed: 0:03:23.\n",
      "  Batch 1,440  of  2,787.    Elapsed: 0:03:29.\n",
      "  Batch 1,480  of  2,787.    Elapsed: 0:03:35.\n",
      "  Batch 1,520  of  2,787.    Elapsed: 0:03:40.\n",
      "  Batch 1,560  of  2,787.    Elapsed: 0:03:46.\n",
      "  Batch 1,600  of  2,787.    Elapsed: 0:03:52.\n",
      "  Batch 1,640  of  2,787.    Elapsed: 0:03:58.\n",
      "  Batch 1,680  of  2,787.    Elapsed: 0:04:04.\n",
      "  Batch 1,720  of  2,787.    Elapsed: 0:04:10.\n",
      "  Batch 1,760  of  2,787.    Elapsed: 0:04:15.\n",
      "  Batch 1,800  of  2,787.    Elapsed: 0:04:21.\n",
      "  Batch 1,840  of  2,787.    Elapsed: 0:04:27.\n",
      "  Batch 1,880  of  2,787.    Elapsed: 0:04:33.\n",
      "  Batch 1,920  of  2,787.    Elapsed: 0:04:39.\n",
      "  Batch 1,960  of  2,787.    Elapsed: 0:04:44.\n",
      "  Batch 2,000  of  2,787.    Elapsed: 0:04:50.\n",
      "  Batch 2,040  of  2,787.    Elapsed: 0:04:56.\n",
      "  Batch 2,080  of  2,787.    Elapsed: 0:05:02.\n",
      "  Batch 2,120  of  2,787.    Elapsed: 0:05:08.\n",
      "  Batch 2,160  of  2,787.    Elapsed: 0:05:13.\n",
      "  Batch 2,200  of  2,787.    Elapsed: 0:05:19.\n",
      "  Batch 2,240  of  2,787.    Elapsed: 0:05:25.\n",
      "  Batch 2,280  of  2,787.    Elapsed: 0:05:31.\n",
      "  Batch 2,320  of  2,787.    Elapsed: 0:05:37.\n",
      "  Batch 2,360  of  2,787.    Elapsed: 0:05:42.\n",
      "  Batch 2,400  of  2,787.    Elapsed: 0:05:48.\n",
      "  Batch 2,440  of  2,787.    Elapsed: 0:05:54.\n",
      "  Batch 2,480  of  2,787.    Elapsed: 0:06:00.\n",
      "  Batch 2,520  of  2,787.    Elapsed: 0:06:06.\n",
      "  Batch 2,560  of  2,787.    Elapsed: 0:06:11.\n",
      "  Batch 2,600  of  2,787.    Elapsed: 0:06:17.\n",
      "  Batch 2,640  of  2,787.    Elapsed: 0:06:23.\n",
      "  Batch 2,680  of  2,787.    Elapsed: 0:06:29.\n",
      "  Batch 2,720  of  2,787.    Elapsed: 0:06:35.\n",
      "  Batch 2,760  of  2,787.    Elapsed: 0:06:40.\n",
      "\n",
      "  Average training loss: 0.91\n",
      "  Training epcoh took: 0:06:44\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.57\n",
      "  Validation Loss: 0.90\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  2,787.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  2,787.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  2,787.    Elapsed: 0:00:18.\n",
      "  Batch   160  of  2,787.    Elapsed: 0:00:23.\n",
      "  Batch   200  of  2,787.    Elapsed: 0:00:29.\n",
      "  Batch   240  of  2,787.    Elapsed: 0:00:35.\n",
      "  Batch   280  of  2,787.    Elapsed: 0:00:41.\n",
      "  Batch   320  of  2,787.    Elapsed: 0:00:47.\n",
      "  Batch   360  of  2,787.    Elapsed: 0:00:52.\n",
      "  Batch   400  of  2,787.    Elapsed: 0:00:58.\n",
      "  Batch   440  of  2,787.    Elapsed: 0:01:04.\n",
      "  Batch   480  of  2,787.    Elapsed: 0:01:10.\n",
      "  Batch   520  of  2,787.    Elapsed: 0:01:16.\n",
      "  Batch   560  of  2,787.    Elapsed: 0:01:21.\n",
      "  Batch   600  of  2,787.    Elapsed: 0:01:27.\n",
      "  Batch   640  of  2,787.    Elapsed: 0:01:33.\n",
      "  Batch   680  of  2,787.    Elapsed: 0:01:39.\n",
      "  Batch   720  of  2,787.    Elapsed: 0:01:45.\n",
      "  Batch   760  of  2,787.    Elapsed: 0:01:50.\n",
      "  Batch   800  of  2,787.    Elapsed: 0:01:56.\n",
      "  Batch   840  of  2,787.    Elapsed: 0:02:02.\n",
      "  Batch   880  of  2,787.    Elapsed: 0:02:08.\n",
      "  Batch   920  of  2,787.    Elapsed: 0:02:14.\n",
      "  Batch   960  of  2,787.    Elapsed: 0:02:19.\n",
      "  Batch 1,000  of  2,787.    Elapsed: 0:02:25.\n",
      "  Batch 1,040  of  2,787.    Elapsed: 0:02:31.\n",
      "  Batch 1,080  of  2,787.    Elapsed: 0:02:37.\n",
      "  Batch 1,120  of  2,787.    Elapsed: 0:02:43.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,160  of  2,787.    Elapsed: 0:02:48.\n",
      "  Batch 1,200  of  2,787.    Elapsed: 0:02:54.\n",
      "  Batch 1,240  of  2,787.    Elapsed: 0:03:00.\n",
      "  Batch 1,280  of  2,787.    Elapsed: 0:03:06.\n",
      "  Batch 1,320  of  2,787.    Elapsed: 0:03:12.\n",
      "  Batch 1,360  of  2,787.    Elapsed: 0:03:17.\n",
      "  Batch 1,400  of  2,787.    Elapsed: 0:03:23.\n",
      "  Batch 1,440  of  2,787.    Elapsed: 0:03:29.\n",
      "  Batch 1,480  of  2,787.    Elapsed: 0:03:35.\n",
      "  Batch 1,520  of  2,787.    Elapsed: 0:03:41.\n",
      "  Batch 1,560  of  2,787.    Elapsed: 0:03:47.\n",
      "  Batch 1,600  of  2,787.    Elapsed: 0:03:52.\n",
      "  Batch 1,640  of  2,787.    Elapsed: 0:03:58.\n",
      "  Batch 1,680  of  2,787.    Elapsed: 0:04:04.\n",
      "  Batch 1,720  of  2,787.    Elapsed: 0:04:10.\n",
      "  Batch 1,760  of  2,787.    Elapsed: 0:04:15.\n",
      "  Batch 1,800  of  2,787.    Elapsed: 0:04:21.\n",
      "  Batch 1,840  of  2,787.    Elapsed: 0:04:27.\n",
      "  Batch 1,880  of  2,787.    Elapsed: 0:04:33.\n",
      "  Batch 1,920  of  2,787.    Elapsed: 0:04:39.\n",
      "  Batch 1,960  of  2,787.    Elapsed: 0:04:44.\n",
      "  Batch 2,000  of  2,787.    Elapsed: 0:04:50.\n",
      "  Batch 2,040  of  2,787.    Elapsed: 0:04:56.\n",
      "  Batch 2,080  of  2,787.    Elapsed: 0:05:02.\n",
      "  Batch 2,120  of  2,787.    Elapsed: 0:05:08.\n",
      "  Batch 2,160  of  2,787.    Elapsed: 0:05:13.\n",
      "  Batch 2,200  of  2,787.    Elapsed: 0:05:19.\n",
      "  Batch 2,240  of  2,787.    Elapsed: 0:05:25.\n",
      "  Batch 2,280  of  2,787.    Elapsed: 0:05:31.\n",
      "  Batch 2,320  of  2,787.    Elapsed: 0:05:37.\n",
      "  Batch 2,360  of  2,787.    Elapsed: 0:05:42.\n",
      "  Batch 2,400  of  2,787.    Elapsed: 0:05:48.\n",
      "  Batch 2,440  of  2,787.    Elapsed: 0:05:54.\n",
      "  Batch 2,480  of  2,787.    Elapsed: 0:06:00.\n",
      "  Batch 2,520  of  2,787.    Elapsed: 0:06:06.\n",
      "  Batch 2,560  of  2,787.    Elapsed: 0:06:12.\n",
      "  Batch 2,600  of  2,787.    Elapsed: 0:06:17.\n",
      "  Batch 2,640  of  2,787.    Elapsed: 0:06:23.\n",
      "  Batch 2,680  of  2,787.    Elapsed: 0:06:29.\n",
      "  Batch 2,720  of  2,787.    Elapsed: 0:06:35.\n",
      "  Batch 2,760  of  2,787.    Elapsed: 0:06:41.\n",
      "\n",
      "  Average training loss: 0.88\n",
      "  Training epcoh took: 0:06:44\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.57\n",
      "  Validation Loss: 0.90\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:20:47 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "xlm_training_stats = training_helper(xlm_model, train_dataloader, val_dataloader, optimizer_xlm, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/pytorch/1.5/install/3.7/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# save model\n",
    "pickle.dump(xlm_model, open(\"./xlm_arabic.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0:06:45</td>\n",
       "      <td>0:00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0:06:44</td>\n",
       "      <td>0:00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0:06:44</td>\n",
       "      <td>0:00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.97         0.92           0.55       0:06:45         0:00:11\n",
       "2               0.91         0.90           0.57       0:06:44         0:00:11\n",
       "3               0.88         0.90           0.57       0:06:44         0:00:11"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show training stats\n",
    "pd.set_option('precision', 2)\n",
    "df_stats = pd.DataFrame(data=xlm_training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr4/cs640/reardonc/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# test on arabic tweets\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "test_labels = []\n",
    "\n",
    "# For every sentence...\n",
    "for idx, row in a_test_df.iterrows():\n",
    "    tw = str(row['preprocessed'])\n",
    "    if row['sentiment'] == \"positive\":\n",
    "        label = 1\n",
    "    elif row['sentiment'] == \"negative\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 2\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tw,                      \n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )    \n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "    test_labels.append(label)\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 16  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 3,353 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "xlm_model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        result = xlm_model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           return_dict=True)\n",
    "        \n",
    "    logits = result.logits\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy:  0.6215608465608465\n"
     ]
    }
   ],
   "source": [
    "xlm_accuracies = []\n",
    "for i in range(len(predictions)):\n",
    "    xlm_accuracies += [flat_accuracy(predictions[i], true_labels[i])]\n",
    "\n",
    "xlm_mean = sum(xlm_accuracies)/len(xlm_accuracies)\n",
    "#print('Accuracies for each test batch: ', xlm_accuracies)\n",
    "print('Average test accuracy: ', xlm_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
