{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Fine-tuning & Testing XLM-Roberta on English-translated & English Data </h1>\n",
    "Carley Reardon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Train Tweets: (21124, 4)\n",
      "                   id sentiment  \\\n",
      "0  264183816548130816  positive   \n",
      "1  263405084770172928  negative   \n",
      "2  262163168678248449  negative   \n",
      "3  264249301910310912  negative   \n",
      "4  262682041215234048   neutral   \n",
      "\n",
      "                                            original  \\\n",
      "0  Gas by my house hit $3.39!!!! I\\u2019m going t...   \n",
      "1  Theo Walcott is still shit\\u002c watch Rafa an...   \n",
      "2  its not that I\\u2019m a GSP fan\\u002c i just h...   \n",
      "3  Iranian general says Israel\\u2019s Iron Dome c...   \n",
      "4  Tehran\\u002c Mon Amour: Obama Tried to Establi...   \n",
      "\n",
      "                                        preprocessed  \n",
      "0  gas house hit $ 3.39 i\\u2019 m going chapel hi...  \n",
      "1  theo walcott shit\\u002c watch rafa johnny deal...  \n",
      "2  i\\u2019 m gsp fan\\u002c hate nick diaz can\\u20...  \n",
      "3  iranian general says israel\\u2019s iron dome c...  \n",
      "4  tehran\\u002c mon amour obama tried establish t...  \n",
      "\n",
      "English Test Tweets: (28333, 4)\n",
      "                   id sentiment  \\\n",
      "0  264238274963451904  positive   \n",
      "1  218775148495515649  positive   \n",
      "2  258965201766998017   neutral   \n",
      "3  262926411352903682  negative   \n",
      "4  171874368908050432   neutral   \n",
      "\n",
      "                                            original  \\\n",
      "0  @jjuueellzz down in the Atlantic city, ventnor...   \n",
      "1  Musical awareness: Great Big Beautiful Tomorro...   \n",
      "2  On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...   \n",
      "3  Kapan sih lo ngebuktiin,jan ngomong doang Susa...   \n",
      "4  Excuse the connectivity of this live stream, f...   \n",
      "\n",
      "                                        preprocessed  \n",
      "0  atlantic city ventnor margate ocean city area ...  \n",
      "1  musical awareness great big beautiful tomorrow...  \n",
      "2  radio786 100.4fm 7:10 fri oct 19 labour analys...  \n",
      "3  kapan sih lo ngebuktiin jan ngomong doang susa...  \n",
      "4  excuse connectivity live stream baba amr activ...  \n",
      "\n",
      "Arabic-to-English Test Tweets: (3353, 6)\n",
      "   Unnamed: 0                  id sentiment  \\\n",
      "0           0  783555835494592513  positive   \n",
      "1           1  783582397166125056  positive   \n",
      "2           2  783592390728769536  positive   \n",
      "3           3  783597390070685696  positive   \n",
      "4           4  783617442031472640   neutral   \n",
      "\n",
      "                                            original  \\\n",
      "0  إجبار أبل على التعاون على فك شفرة اجهزتها http...   \n",
      "1  RT @20fourMedia: #غوغل تتحدى أبل وأمازون بأجهز...   \n",
      "2  جوجل تنافس أبل وسامسونج بهاتف جديد https://t.c...   \n",
      "3  رئيس شركة أبل: الواقع المعزز سيصبح أهم من الطع...   \n",
      "4  ساعة أبل في الأسواق مرة أخرى https://t.co/dY2x...   \n",
      "\n",
      "                                      preprocessed  \\\n",
      "0        إجبار أبل على التعاون على فك شفرة اجهزتها   \n",
      "1              غوغل تتحدى أبل وأمازون بأجهزة جديدة   \n",
      "2               جوجل تنافس أبل وسامسونج بهاتف جديد   \n",
      "3  رئيس شركة أبل الواقع المعزز سيصبح أهم من الطعام   \n",
      "4                     ساعة أبل في الأسواق مرة أخرى   \n",
      "\n",
      "                                          translated  \n",
      "0  To force Apple to cooperate in deciphering her...  \n",
      "1  Google is challenging Apple and Amazon with ne...  \n",
      "2  Google is competing with Apple and Samsung wit...  \n",
      "3  Apple's president, enhanced reality will becom...  \n",
      "4                  Apple hour's in the market again.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# first import our translated and original data\n",
    "e_prep_path = \"/projectnb/cs505/reardonc/CS505-Spring-MultiLingual-Twitter-Classification/data/preprocessed/english/\"\n",
    "a_transl_path = \"/projectnb/cs505/reardonc/CS505-Spring-MultiLingual-Twitter-Classification/data/translated/arabic/\"\n",
    "\n",
    "e_train_df = pd.read_csv(e_prep_path+\"english_train_tweets.csv\")\n",
    "e_test_df = pd.read_csv(e_prep_path+\"english_test_tweets.csv\")\n",
    "a_test_df = pd.read_csv(a_transl_path+\"arabic_test_tweets.csv\")\n",
    "\n",
    "print(\"English Train Tweets: \"+str(e_train_df.shape))\n",
    "print(e_train_df.head(5))\n",
    "print(\"\")\n",
    "print(\"English Test Tweets: \"+str(e_test_df.shape))\n",
    "print(e_test_df.head(5))\n",
    "print(\"\")\n",
    "print(\"Arabic-to-English Test Tweets: \"+str(a_test_df.shape))\n",
    "print(a_test_df.head(5))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (4.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (4.46.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (2020.5.14)\n",
      "Requirement already satisfied: sacremoses in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: packaging in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: requests in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: filelock in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: zipp>=0.5 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.1.0)\n",
      "Requirement already satisfied: six in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: click in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from sacremoses->transformers) (0.15.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (0.1.95)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "# set up data for model\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=True)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "\n",
    "for idx, row in e_train_df.iterrows():\n",
    "    #print(row)\n",
    "    tw = str(row['preprocessed'])\n",
    "    if row['sentiment'] == \"positive\":\n",
    "        label = 1\n",
    "    elif row['sentiment'] == \"negative\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 2\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tw,                      \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 128,\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )   \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    labels.append(label)\n",
    "\n",
    "for idx, row in e_test_df.iterrows():\n",
    "    tw = str(row['preprocessed'])\n",
    "    if row['sentiment'] == \"positive\":\n",
    "        label = 1\n",
    "    elif row['sentiment'] == \"negative\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 2\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tw,                      \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 128,\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )   \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    labels.append(label)\n",
    "    \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44,511 training samples\n",
      "4,946 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "batch_size = 16\n",
    " \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "# Load XLM-R\n",
    "xlm_model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\", # Use the base XLM-R model, with an uncased vocab.\n",
    "    num_labels = 3, # The number of output labels--3 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "xlm_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "optimizer_xlm = AdamW(xlm_model.parameters(),\n",
    "                  lr = 5e-6,\n",
    "                  eps = 1e-8\n",
    "                )\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer_xlm, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def training_helper(model, train_dataloader, validation_dataloader, optimizer, scheduler, \n",
    "                    seed_val=42, epochs=3, ):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    training_stats = []\n",
    "\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            result = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels,\n",
    "                        return_dict=True)\n",
    "\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            with torch.no_grad():        \n",
    "                result = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels,\n",
    "                            return_dict=True)\n",
    "\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  2,782.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  2,782.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  2,782.    Elapsed: 0:00:17.\n",
      "  Batch   160  of  2,782.    Elapsed: 0:00:23.\n",
      "  Batch   200  of  2,782.    Elapsed: 0:00:29.\n",
      "  Batch   240  of  2,782.    Elapsed: 0:00:35.\n",
      "  Batch   280  of  2,782.    Elapsed: 0:00:40.\n",
      "  Batch   320  of  2,782.    Elapsed: 0:00:46.\n",
      "  Batch   360  of  2,782.    Elapsed: 0:00:52.\n",
      "  Batch   400  of  2,782.    Elapsed: 0:00:58.\n",
      "  Batch   440  of  2,782.    Elapsed: 0:01:03.\n",
      "  Batch   480  of  2,782.    Elapsed: 0:01:09.\n",
      "  Batch   520  of  2,782.    Elapsed: 0:01:15.\n",
      "  Batch   560  of  2,782.    Elapsed: 0:01:21.\n",
      "  Batch   600  of  2,782.    Elapsed: 0:01:27.\n",
      "  Batch   640  of  2,782.    Elapsed: 0:01:32.\n",
      "  Batch   680  of  2,782.    Elapsed: 0:01:38.\n",
      "  Batch   720  of  2,782.    Elapsed: 0:01:44.\n",
      "  Batch   760  of  2,782.    Elapsed: 0:01:50.\n",
      "  Batch   800  of  2,782.    Elapsed: 0:01:56.\n",
      "  Batch   840  of  2,782.    Elapsed: 0:02:01.\n",
      "  Batch   880  of  2,782.    Elapsed: 0:02:07.\n",
      "  Batch   920  of  2,782.    Elapsed: 0:02:13.\n",
      "  Batch   960  of  2,782.    Elapsed: 0:02:19.\n",
      "  Batch 1,000  of  2,782.    Elapsed: 0:02:24.\n",
      "  Batch 1,040  of  2,782.    Elapsed: 0:02:30.\n",
      "  Batch 1,080  of  2,782.    Elapsed: 0:02:36.\n",
      "  Batch 1,120  of  2,782.    Elapsed: 0:02:42.\n",
      "  Batch 1,160  of  2,782.    Elapsed: 0:02:48.\n",
      "  Batch 1,200  of  2,782.    Elapsed: 0:02:54.\n",
      "  Batch 1,240  of  2,782.    Elapsed: 0:02:59.\n",
      "  Batch 1,280  of  2,782.    Elapsed: 0:03:05.\n",
      "  Batch 1,320  of  2,782.    Elapsed: 0:03:11.\n",
      "  Batch 1,360  of  2,782.    Elapsed: 0:03:17.\n",
      "  Batch 1,400  of  2,782.    Elapsed: 0:03:22.\n",
      "  Batch 1,440  of  2,782.    Elapsed: 0:03:28.\n",
      "  Batch 1,480  of  2,782.    Elapsed: 0:03:34.\n",
      "  Batch 1,520  of  2,782.    Elapsed: 0:03:40.\n",
      "  Batch 1,560  of  2,782.    Elapsed: 0:03:46.\n",
      "  Batch 1,600  of  2,782.    Elapsed: 0:03:51.\n",
      "  Batch 1,640  of  2,782.    Elapsed: 0:03:57.\n",
      "  Batch 1,680  of  2,782.    Elapsed: 0:04:03.\n",
      "  Batch 1,720  of  2,782.    Elapsed: 0:04:09.\n",
      "  Batch 1,760  of  2,782.    Elapsed: 0:04:14.\n",
      "  Batch 1,800  of  2,782.    Elapsed: 0:04:20.\n",
      "  Batch 1,840  of  2,782.    Elapsed: 0:04:26.\n",
      "  Batch 1,880  of  2,782.    Elapsed: 0:04:32.\n",
      "  Batch 1,920  of  2,782.    Elapsed: 0:04:38.\n",
      "  Batch 1,960  of  2,782.    Elapsed: 0:04:44.\n",
      "  Batch 2,000  of  2,782.    Elapsed: 0:04:49.\n",
      "  Batch 2,040  of  2,782.    Elapsed: 0:04:55.\n",
      "  Batch 2,080  of  2,782.    Elapsed: 0:05:01.\n",
      "  Batch 2,120  of  2,782.    Elapsed: 0:05:07.\n",
      "  Batch 2,160  of  2,782.    Elapsed: 0:05:13.\n",
      "  Batch 2,200  of  2,782.    Elapsed: 0:05:18.\n",
      "  Batch 2,240  of  2,782.    Elapsed: 0:05:24.\n",
      "  Batch 2,280  of  2,782.    Elapsed: 0:05:30.\n",
      "  Batch 2,320  of  2,782.    Elapsed: 0:05:36.\n",
      "  Batch 2,360  of  2,782.    Elapsed: 0:05:42.\n",
      "  Batch 2,400  of  2,782.    Elapsed: 0:05:48.\n",
      "  Batch 2,440  of  2,782.    Elapsed: 0:05:53.\n",
      "  Batch 2,480  of  2,782.    Elapsed: 0:05:59.\n",
      "  Batch 2,520  of  2,782.    Elapsed: 0:06:05.\n",
      "  Batch 2,560  of  2,782.    Elapsed: 0:06:11.\n",
      "  Batch 2,600  of  2,782.    Elapsed: 0:06:17.\n",
      "  Batch 2,640  of  2,782.    Elapsed: 0:06:23.\n",
      "  Batch 2,680  of  2,782.    Elapsed: 0:06:28.\n",
      "  Batch 2,720  of  2,782.    Elapsed: 0:06:34.\n",
      "  Batch 2,760  of  2,782.    Elapsed: 0:06:40.\n",
      "\n",
      "  Average training loss: 0.83\n",
      "  Training epcoh took: 0:06:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.66\n",
      "  Validation Loss: 0.76\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  2,782.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  2,782.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  2,782.    Elapsed: 0:00:17.\n",
      "  Batch   160  of  2,782.    Elapsed: 0:00:23.\n",
      "  Batch   200  of  2,782.    Elapsed: 0:00:29.\n",
      "  Batch   240  of  2,782.    Elapsed: 0:00:35.\n",
      "  Batch   280  of  2,782.    Elapsed: 0:00:41.\n",
      "  Batch   320  of  2,782.    Elapsed: 0:00:46.\n",
      "  Batch   360  of  2,782.    Elapsed: 0:00:52.\n",
      "  Batch   400  of  2,782.    Elapsed: 0:00:58.\n",
      "  Batch   440  of  2,782.    Elapsed: 0:01:04.\n",
      "  Batch   480  of  2,782.    Elapsed: 0:01:10.\n",
      "  Batch   520  of  2,782.    Elapsed: 0:01:15.\n",
      "  Batch   560  of  2,782.    Elapsed: 0:01:21.\n",
      "  Batch   600  of  2,782.    Elapsed: 0:01:27.\n",
      "  Batch   640  of  2,782.    Elapsed: 0:01:33.\n",
      "  Batch   680  of  2,782.    Elapsed: 0:01:39.\n",
      "  Batch   720  of  2,782.    Elapsed: 0:01:44.\n",
      "  Batch   760  of  2,782.    Elapsed: 0:01:50.\n",
      "  Batch   800  of  2,782.    Elapsed: 0:01:56.\n",
      "  Batch   840  of  2,782.    Elapsed: 0:02:02.\n",
      "  Batch   880  of  2,782.    Elapsed: 0:02:07.\n",
      "  Batch   920  of  2,782.    Elapsed: 0:02:13.\n",
      "  Batch   960  of  2,782.    Elapsed: 0:02:19.\n",
      "  Batch 1,000  of  2,782.    Elapsed: 0:02:25.\n",
      "  Batch 1,040  of  2,782.    Elapsed: 0:02:31.\n",
      "  Batch 1,080  of  2,782.    Elapsed: 0:02:36.\n",
      "  Batch 1,120  of  2,782.    Elapsed: 0:02:42.\n",
      "  Batch 1,160  of  2,782.    Elapsed: 0:02:48.\n",
      "  Batch 1,200  of  2,782.    Elapsed: 0:02:54.\n",
      "  Batch 1,240  of  2,782.    Elapsed: 0:03:00.\n",
      "  Batch 1,280  of  2,782.    Elapsed: 0:03:05.\n",
      "  Batch 1,320  of  2,782.    Elapsed: 0:03:11.\n",
      "  Batch 1,360  of  2,782.    Elapsed: 0:03:17.\n",
      "  Batch 1,400  of  2,782.    Elapsed: 0:03:23.\n",
      "  Batch 1,440  of  2,782.    Elapsed: 0:03:29.\n",
      "  Batch 1,480  of  2,782.    Elapsed: 0:03:35.\n",
      "  Batch 1,520  of  2,782.    Elapsed: 0:03:40.\n",
      "  Batch 1,560  of  2,782.    Elapsed: 0:03:46.\n",
      "  Batch 1,600  of  2,782.    Elapsed: 0:03:52.\n",
      "  Batch 1,640  of  2,782.    Elapsed: 0:03:58.\n",
      "  Batch 1,680  of  2,782.    Elapsed: 0:04:04.\n",
      "  Batch 1,720  of  2,782.    Elapsed: 0:04:09.\n",
      "  Batch 1,760  of  2,782.    Elapsed: 0:04:15.\n",
      "  Batch 1,800  of  2,782.    Elapsed: 0:04:21.\n",
      "  Batch 1,840  of  2,782.    Elapsed: 0:04:27.\n",
      "  Batch 1,880  of  2,782.    Elapsed: 0:04:33.\n",
      "  Batch 1,920  of  2,782.    Elapsed: 0:04:38.\n",
      "  Batch 1,960  of  2,782.    Elapsed: 0:04:44.\n",
      "  Batch 2,000  of  2,782.    Elapsed: 0:04:50.\n",
      "  Batch 2,040  of  2,782.    Elapsed: 0:04:56.\n",
      "  Batch 2,080  of  2,782.    Elapsed: 0:05:02.\n",
      "  Batch 2,120  of  2,782.    Elapsed: 0:05:07.\n",
      "  Batch 2,160  of  2,782.    Elapsed: 0:05:13.\n",
      "  Batch 2,200  of  2,782.    Elapsed: 0:05:19.\n",
      "  Batch 2,240  of  2,782.    Elapsed: 0:05:25.\n",
      "  Batch 2,280  of  2,782.    Elapsed: 0:05:31.\n",
      "  Batch 2,320  of  2,782.    Elapsed: 0:05:36.\n",
      "  Batch 2,360  of  2,782.    Elapsed: 0:05:42.\n",
      "  Batch 2,400  of  2,782.    Elapsed: 0:05:48.\n",
      "  Batch 2,440  of  2,782.    Elapsed: 0:05:54.\n",
      "  Batch 2,480  of  2,782.    Elapsed: 0:06:00.\n",
      "  Batch 2,520  of  2,782.    Elapsed: 0:06:05.\n",
      "  Batch 2,560  of  2,782.    Elapsed: 0:06:11.\n",
      "  Batch 2,600  of  2,782.    Elapsed: 0:06:17.\n",
      "  Batch 2,640  of  2,782.    Elapsed: 0:06:23.\n",
      "  Batch 2,680  of  2,782.    Elapsed: 0:06:29.\n",
      "  Batch 2,720  of  2,782.    Elapsed: 0:06:34.\n",
      "  Batch 2,760  of  2,782.    Elapsed: 0:06:40.\n",
      "\n",
      "  Average training loss: 0.73\n",
      "  Training epcoh took: 0:06:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.66\n",
      "  Validation Loss: 0.76\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  2,782.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  2,782.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  2,782.    Elapsed: 0:00:18.\n",
      "  Batch   160  of  2,782.    Elapsed: 0:00:23.\n",
      "  Batch   200  of  2,782.    Elapsed: 0:00:29.\n",
      "  Batch   240  of  2,782.    Elapsed: 0:00:35.\n",
      "  Batch   280  of  2,782.    Elapsed: 0:00:41.\n",
      "  Batch   320  of  2,782.    Elapsed: 0:00:46.\n",
      "  Batch   360  of  2,782.    Elapsed: 0:00:52.\n",
      "  Batch   400  of  2,782.    Elapsed: 0:00:58.\n",
      "  Batch   440  of  2,782.    Elapsed: 0:01:04.\n",
      "  Batch   480  of  2,782.    Elapsed: 0:01:10.\n",
      "  Batch   520  of  2,782.    Elapsed: 0:01:15.\n",
      "  Batch   560  of  2,782.    Elapsed: 0:01:21.\n",
      "  Batch   600  of  2,782.    Elapsed: 0:01:27.\n",
      "  Batch   640  of  2,782.    Elapsed: 0:01:33.\n",
      "  Batch   680  of  2,782.    Elapsed: 0:01:39.\n",
      "  Batch   720  of  2,782.    Elapsed: 0:01:44.\n",
      "  Batch   760  of  2,782.    Elapsed: 0:01:50.\n",
      "  Batch   800  of  2,782.    Elapsed: 0:01:56.\n",
      "  Batch   840  of  2,782.    Elapsed: 0:02:02.\n",
      "  Batch   880  of  2,782.    Elapsed: 0:02:08.\n",
      "  Batch   920  of  2,782.    Elapsed: 0:02:13.\n",
      "  Batch   960  of  2,782.    Elapsed: 0:02:19.\n",
      "  Batch 1,000  of  2,782.    Elapsed: 0:02:25.\n",
      "  Batch 1,040  of  2,782.    Elapsed: 0:02:31.\n",
      "  Batch 1,080  of  2,782.    Elapsed: 0:02:37.\n",
      "  Batch 1,120  of  2,782.    Elapsed: 0:02:43.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,160  of  2,782.    Elapsed: 0:02:48.\n",
      "  Batch 1,200  of  2,782.    Elapsed: 0:02:54.\n",
      "  Batch 1,240  of  2,782.    Elapsed: 0:03:00.\n",
      "  Batch 1,280  of  2,782.    Elapsed: 0:03:06.\n",
      "  Batch 1,320  of  2,782.    Elapsed: 0:03:12.\n",
      "  Batch 1,360  of  2,782.    Elapsed: 0:03:17.\n",
      "  Batch 1,400  of  2,782.    Elapsed: 0:03:23.\n",
      "  Batch 1,440  of  2,782.    Elapsed: 0:03:29.\n",
      "  Batch 1,480  of  2,782.    Elapsed: 0:03:35.\n",
      "  Batch 1,520  of  2,782.    Elapsed: 0:03:41.\n",
      "  Batch 1,560  of  2,782.    Elapsed: 0:03:46.\n",
      "  Batch 1,600  of  2,782.    Elapsed: 0:03:52.\n",
      "  Batch 1,640  of  2,782.    Elapsed: 0:03:58.\n",
      "  Batch 1,680  of  2,782.    Elapsed: 0:04:04.\n",
      "  Batch 1,720  of  2,782.    Elapsed: 0:04:10.\n",
      "  Batch 1,760  of  2,782.    Elapsed: 0:04:15.\n",
      "  Batch 1,800  of  2,782.    Elapsed: 0:04:21.\n",
      "  Batch 1,840  of  2,782.    Elapsed: 0:04:27.\n",
      "  Batch 1,880  of  2,782.    Elapsed: 0:04:33.\n",
      "  Batch 1,920  of  2,782.    Elapsed: 0:04:39.\n",
      "  Batch 1,960  of  2,782.    Elapsed: 0:04:44.\n",
      "  Batch 2,000  of  2,782.    Elapsed: 0:04:50.\n",
      "  Batch 2,040  of  2,782.    Elapsed: 0:04:56.\n",
      "  Batch 2,080  of  2,782.    Elapsed: 0:05:02.\n",
      "  Batch 2,120  of  2,782.    Elapsed: 0:05:08.\n",
      "  Batch 2,160  of  2,782.    Elapsed: 0:05:13.\n",
      "  Batch 2,200  of  2,782.    Elapsed: 0:05:19.\n",
      "  Batch 2,240  of  2,782.    Elapsed: 0:05:25.\n",
      "  Batch 2,280  of  2,782.    Elapsed: 0:05:31.\n",
      "  Batch 2,320  of  2,782.    Elapsed: 0:05:37.\n",
      "  Batch 2,360  of  2,782.    Elapsed: 0:05:42.\n",
      "  Batch 2,400  of  2,782.    Elapsed: 0:05:48.\n",
      "  Batch 2,440  of  2,782.    Elapsed: 0:05:54.\n",
      "  Batch 2,480  of  2,782.    Elapsed: 0:06:00.\n",
      "  Batch 2,520  of  2,782.    Elapsed: 0:06:06.\n",
      "  Batch 2,560  of  2,782.    Elapsed: 0:06:11.\n",
      "  Batch 2,600  of  2,782.    Elapsed: 0:06:17.\n",
      "  Batch 2,640  of  2,782.    Elapsed: 0:06:23.\n",
      "  Batch 2,680  of  2,782.    Elapsed: 0:06:29.\n",
      "  Batch 2,720  of  2,782.    Elapsed: 0:06:34.\n",
      "  Batch 2,760  of  2,782.    Elapsed: 0:06:40.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epcoh took: 0:06:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.67\n",
      "  Validation Loss: 0.74\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:20:43 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "xlm_training_stats = training_helper(xlm_model, train_dataloader, val_dataloader, optimizer_xlm, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/pytorch/1.5/install/3.7/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# save model\n",
    "pickle.dump(xlm_model, open(\"./xlm_english.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0:06:43</td>\n",
       "      <td>0:00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0:06:43</td>\n",
       "      <td>0:00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0:06:43</td>\n",
       "      <td>0:00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.83         0.76           0.66       0:06:43         0:00:11\n",
       "2               0.73         0.76           0.66       0:06:43         0:00:11\n",
       "3               0.70         0.74           0.67       0:06:43         0:00:11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show training stats\n",
    "pd.set_option('precision', 2)\n",
    "df_stats = pd.DataFrame(data=xlm_training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr4/cs640/reardonc/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# test on arabic tweets\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "test_labels = []\n",
    "\n",
    "# For every sentence...\n",
    "for idx, row in a_test_df.iterrows():\n",
    "    tw = str(row['translated'])\n",
    "    if row['sentiment'] == \"positive\":\n",
    "        label = 1\n",
    "    elif row['sentiment'] == \"negative\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 2\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tw,                      \n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )    \n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "    test_labels.append(label)\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 16  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 3,353 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "xlm_model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        result = xlm_model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           return_dict=True)\n",
    "        \n",
    "    logits = result.logits\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy:  0.5921626984126984\n"
     ]
    }
   ],
   "source": [
    "xlm_accuracies = []\n",
    "for i in range(len(predictions)):\n",
    "    xlm_accuracies += [flat_accuracy(predictions[i], true_labels[i])]\n",
    "\n",
    "xlm_mean = sum(xlm_accuracies)/len(xlm_accuracies)\n",
    "#print('Accuracies for each test batch: ', xlm_accuracies)\n",
    "print('Average test accuracy: ', xlm_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
