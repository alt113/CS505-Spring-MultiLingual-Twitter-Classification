{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Machine Translation of English & Arabic Tweets </h1>\n",
    "<h3> Generating translations for both English to Arabic, and Arabic to English </h3>\n",
    "\n",
    "Carley Reardon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Train Tweets: (16090, 4)\n",
      "                   id sentiment  \\\n",
      "0  264183816548130816  positive   \n",
      "1  263405084770172928  negative   \n",
      "2  262163168678248449  negative   \n",
      "3  264249301910310912  negative   \n",
      "4  262682041215234048   neutral   \n",
      "\n",
      "                                            original  \\\n",
      "0  Gas by my house hit $3.39!!!! I\\u2019m going t...   \n",
      "1  Theo Walcott is still shit\\u002c watch Rafa an...   \n",
      "2  its not that I\\u2019m a GSP fan\\u002c i just h...   \n",
      "3  Iranian general says Israel\\u2019s Iron Dome c...   \n",
      "4  Tehran\\u002c Mon Amour: Obama Tried to Establi...   \n",
      "\n",
      "                                        preprocessed  \n",
      "0  gas house hit $ 3.39 i\\u2019 m going chapel hi...  \n",
      "1  theo walcott shit\\u002c watch rafa johnny deal...  \n",
      "2  i\\u2019 m gsp fan\\u002c hate nick diaz can\\u20...  \n",
      "3  iranian general says israel\\u2019s iron dome c...  \n",
      "4  tehran\\u002c mon amour obama tried establish t...  \n",
      "\n",
      "English Test Tweets: (28422, 4)\n",
      "             id sentiment                                           original  \\\n",
      "0  2.642380e+17  positive  @jjuueellzz down in the Atlantic city, ventnor...   \n",
      "1  2.187750e+17  positive  Musical awareness: Great Big Beautiful Tomorro...   \n",
      "2  2.589650e+17   neutral  On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...   \n",
      "3  2.629260e+17  negative  Kapan sih lo ngebuktiin,jan ngomong doang Susa...   \n",
      "4  1.718740e+17   neutral  Excuse the connectivity of this live stream, f...   \n",
      "\n",
      "                                        preprocessed  \n",
      "0  atlantic city ventnor margate ocean city area ...  \n",
      "1  musical awareness great big beautiful tomorrow...  \n",
      "2  radio786 100.4fm 7:10 fri oct 19 labour analys...  \n",
      "3  kapan sih lo ngebuktiin jan ngomong doang susa...  \n",
      "4  excuse connectivity live stream baba amr activ...  \n",
      "\n",
      "Arabic Test Tweets: (3353, 4)\n",
      "                   id sentiment  \\\n",
      "0  783555835494592513  positive   \n",
      "1  783582397166125056  positive   \n",
      "2  783592390728769536  positive   \n",
      "3  783597390070685696  positive   \n",
      "4  783617442031472640   neutral   \n",
      "\n",
      "                                            original  \\\n",
      "0  إجبار أبل على التعاون على فك شفرة اجهزتها http...   \n",
      "1  RT @20fourMedia: #غوغل تتحدى أبل وأمازون بأجهز...   \n",
      "2  جوجل تنافس أبل وسامسونج بهاتف جديد https://t.c...   \n",
      "3  رئيس شركة أبل: الواقع المعزز سيصبح أهم من الطع...   \n",
      "4  ساعة أبل في الأسواق مرة أخرى https://t.co/dY2x...   \n",
      "\n",
      "                                      preprocessed  \n",
      "0        إجبار أبل على التعاون على فك شفرة اجهزتها  \n",
      "1              غوغل تتحدى أبل وأمازون بأجهزة جديدة  \n",
      "2               جوجل تنافس أبل وسامسونج بهاتف جديد  \n",
      "3  رئيس شركة أبل الواقع المعزز سيصبح أهم من الطعام  \n",
      "4                     ساعة أبل في الأسواق مرة أخرى  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# first import our preprocessed tweets\n",
    "e_prep_path = \"/projectnb/cs505/reardonc/CS505-Spring-MultiLingual-Twitter-Classification/data/preprocessed/english/\"\n",
    "a_prep_path = \"/projectnb/cs505/reardonc/CS505-Spring-MultiLingual-Twitter-Classification/data/preprocessed/arabic/\"\n",
    "\n",
    "e_train_df = pd.read_csv(e_prep_path+\"english_train_tweets.csv\")\n",
    "e_test_df = pd.read_csv(e_prep_path+\"english_test_tweets.csv\")\n",
    "a_test_df = pd.read_csv(a_prep_path+\"arabic_test_tweets.csv\")\n",
    "\n",
    "print(\"English Train Tweets: \"+str(e_train_df.shape))\n",
    "print(e_train_df.head(5))\n",
    "print(\"\")\n",
    "print(\"English Test Tweets: \"+str(e_test_df.shape))\n",
    "print(e_test_df.head(5))\n",
    "print(\"\")\n",
    "print(\"Arabic Test Tweets: \"+str(a_test_df.shape))\n",
    "print(a_test_df.head(5))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (21.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (4.5.1)\n",
      "Requirement already satisfied: packaging in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (4.46.0)\n",
      "Requirement already satisfied: filelock in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (1.6.0)\n",
      "Requirement already satisfied: requests in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: sacremoses in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (2020.5.14)\n",
      "Requirement already satisfied: zipp>=0.5 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.1.0)\n",
      "Requirement already satisfied: six in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: joblib in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: click in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (4.5.1)\n",
      "Requirement already satisfied: sentencepiece in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (0.1.95)\n",
      "Requirement already satisfied: numpy>=1.17 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: requests in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (4.46.0)\n",
      "Requirement already satisfied: packaging in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (2020.5.14)\n",
      "Requirement already satisfied: importlib-metadata in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: joblib in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: click in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load tokenizers and models\n",
    "tokenizer_ar_en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\", use_fast=False) # Arabic to English\n",
    "tokenizer_en_ar = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\", use_fast=False) # English to Arabic\n",
    "model_ar_en = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\") # Arabic to English\n",
    "model_en_ar = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\") # English to Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating English Train Tweets...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4182fdef2ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocessed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_en_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_en_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_en_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0me_train_transl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.5/install/3.7/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, **model_kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m             )\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m             )\n\u001b[1;32m   1738\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.5/install/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m         )\n\u001b[1;32m   1279\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.5/install/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m         )\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.5/install/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, encoder_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1016\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m                 )\n\u001b[1;32m   1020\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.5/install/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, encoder_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         )\n\u001b[1;32m    424\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.5/install/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0msrc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         assert attn_weights.size() == (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Translate english train tweets\n",
    "model_en_ar.to(device)\n",
    "e_train_transl = []\n",
    "print(\"Translating English Train Tweets...\")\n",
    "for tweet in e_train_df['preprocessed']:\n",
    "    input_ids = tokenizer_en_ar.encode(tweet, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(device)\n",
    "    outputs = model_en_ar.generate(input_ids)\n",
    "    decoded = tokenizer_en_ar.decode(outputs[0], skip_special_tokens=True)\n",
    "    e_train_transl.append(decoded)\n",
    "    if len(e_train_transl) % 200 == 0:\n",
    "        print(\"...tweet no. \"+str(len(e_train_transl))+\"...\")\n",
    "    \n",
    "print(e_train_df['preprocessed'][0])\n",
    "print(e_train_transl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save translated tweets\n",
    "e_train_df['translated'] = e_train_transl\n",
    "print(e_train_df.head())\n",
    "\n",
    "#e_train_outpath = \"/projectnb/cs505/reardonc/CS505-Spring-MultiLingual-Twitter-Classification/data/translated/english/english_train_tweets.csv\"\n",
    "#e_train_df.to_csv(e_train_outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating English Test Tweets...\n",
      "...tweet no. 200...\n",
      "...tweet no. 400...\n",
      "...tweet no. 600...\n",
      "...tweet no. 800...\n",
      "...tweet no. 1000...\n",
      "...tweet no. 1200...\n",
      "...tweet no. 1400...\n",
      "...tweet no. 1600...\n",
      "...tweet no. 1800...\n",
      "...tweet no. 2000...\n",
      "...tweet no. 2200...\n",
      "...tweet no. 2400...\n",
      "...tweet no. 2600...\n",
      "...tweet no. 2800...\n",
      "...tweet no. 3000...\n",
      "...tweet no. 3200...\n",
      "...tweet no. 3400...\n",
      "...tweet no. 3600...\n",
      "...tweet no. 3800...\n",
      "...tweet no. 4000...\n",
      "...tweet no. 4200...\n",
      "...tweet no. 4400...\n",
      "...tweet no. 4600...\n",
      "...tweet no. 4800...\n",
      "...tweet no. 5000...\n",
      "...tweet no. 5200...\n",
      "...tweet no. 5400...\n",
      "...tweet no. 5600...\n",
      "...tweet no. 5800...\n",
      "...tweet no. 6000...\n",
      "...tweet no. 6200...\n",
      "...tweet no. 6400...\n",
      "...tweet no. 6600...\n",
      "...tweet no. 6800...\n",
      "...tweet no. 7000...\n",
      "...tweet no. 7200...\n",
      "...tweet no. 7400...\n",
      "...tweet no. 7600...\n",
      "...tweet no. 7800...\n",
      "...tweet no. 8000...\n",
      "...tweet no. 8200...\n",
      "...tweet no. 8400...\n",
      "...tweet no. 8600...\n",
      "...tweet no. 8800...\n",
      "...tweet no. 9000...\n",
      "...tweet no. 9200...\n",
      "...tweet no. 9400...\n",
      "...tweet no. 9600...\n",
      "...tweet no. 9800...\n",
      "...tweet no. 10000...\n",
      "...tweet no. 10200...\n",
      "...tweet no. 10400...\n",
      "...tweet no. 10600...\n",
      "...tweet no. 10800...\n",
      "...tweet no. 11000...\n",
      "...tweet no. 11200...\n",
      "...tweet no. 11400...\n",
      "...tweet no. 11600...\n",
      "...tweet no. 11800...\n",
      "...tweet no. 12000...\n",
      "...tweet no. 12200...\n",
      "...tweet no. 12400...\n",
      "...tweet no. 12600...\n",
      "...tweet no. 12800...\n",
      "...tweet no. 13000...\n",
      "...tweet no. 13200...\n",
      "...tweet no. 13400...\n",
      "...tweet no. 13600...\n",
      "...tweet no. 13800...\n",
      "...tweet no. 14000...\n",
      "...tweet no. 14200...\n",
      "...tweet no. 14400...\n",
      "...tweet no. 14600...\n",
      "...tweet no. 14800...\n",
      "...tweet no. 15000...\n",
      "...tweet no. 15200...\n",
      "...tweet no. 15400...\n",
      "...tweet no. 15600...\n",
      "...tweet no. 15800...\n",
      "...tweet no. 16000...\n",
      "...tweet no. 16200...\n",
      "...tweet no. 16400...\n",
      "...tweet no. 16600...\n",
      "...tweet no. 16800...\n",
      "...tweet no. 17000...\n",
      "...tweet no. 17200...\n",
      "...tweet no. 17400...\n",
      "...tweet no. 17600...\n",
      "...tweet no. 17800...\n",
      "...tweet no. 18000...\n",
      "...tweet no. 18200...\n",
      "...tweet no. 18400...\n",
      "...tweet no. 18600...\n",
      "...tweet no. 18800...\n",
      "...tweet no. 19000...\n",
      "...tweet no. 19200...\n",
      "...tweet no. 19400...\n",
      "...tweet no. 19600...\n",
      "...tweet no. 19800...\n",
      "...tweet no. 20000...\n",
      "...tweet no. 20200...\n",
      "...tweet no. 20400...\n",
      "...tweet no. 20600...\n",
      "...tweet no. 20800...\n",
      "...tweet no. 21000...\n",
      "...tweet no. 21200...\n",
      "...tweet no. 21400...\n",
      "...tweet no. 21600...\n",
      "...tweet no. 21800...\n",
      "...tweet no. 22000...\n",
      "...tweet no. 22200...\n",
      "...tweet no. 22400...\n",
      "...tweet no. 22600...\n",
      "...tweet no. 22800...\n",
      "...tweet no. 23000...\n",
      "...tweet no. 23200...\n",
      "...tweet no. 23400...\n",
      "...tweet no. 23600...\n",
      "...tweet no. 23800...\n",
      "...tweet no. 24000...\n",
      "...tweet no. 24200...\n",
      "...tweet no. 24400...\n",
      "...tweet no. 24600...\n",
      "...tweet no. 24800...\n",
      "...tweet no. 25000...\n",
      "...tweet no. 25200...\n",
      "...tweet no. 25400...\n",
      "...tweet no. 25600...\n",
      "...tweet no. 25800...\n",
      "...tweet no. 26000...\n",
      "...tweet no. 26200...\n",
      "...tweet no. 26400...\n",
      "...tweet no. 26600...\n",
      "...tweet no. 26800...\n",
      "...tweet no. 27000...\n",
      "...tweet no. 27200...\n",
      "...tweet no. 27400...\n",
      "...tweet no. 27600...\n",
      "...tweet no. 27800...\n",
      "...tweet no. 28000...\n",
      "...tweet no. 28200...\n",
      "...tweet no. 28400...\n",
      "atlantic city ventnor margate ocean city area waiting coordinator hopefully tomorrow\n",
      "منطقة مدينة المحيط الأطلسي في منطقة مدينة المحيط في انتظار المنسق على أمل غدًا\n"
     ]
    }
   ],
   "source": [
    "# Translate english test tweets\n",
    "model_en_ar.to(device)\n",
    "e_test_transl = []\n",
    "print(\"Translating English Test Tweets...\")\n",
    "for tweet in e_test_df['preprocessed']:\n",
    "    input_ids = tokenizer_en_ar.encode(tweet, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(device)\n",
    "    outputs = model_en_ar.generate(input_ids)\n",
    "    decoded = tokenizer_en_ar.decode(outputs[0], skip_special_tokens=True)\n",
    "    e_test_transl.append(decoded)\n",
    "    if len(e_test_transl) % 200 == 0:\n",
    "        print(\"...tweet no. \"+str(len(e_test_transl))+\"...\")\n",
    "        \n",
    "print(e_test_df['preprocessed'][0])\n",
    "print(e_test_transl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save translated tweets\n",
    "e_test_df['translated'] = e_test_transl\n",
    "#print(e_test_df.head())\n",
    "\n",
    "e_test_outpath = \"/projectnb/cs505/reardonc/CS505-Spring-MultiLingual-Twitter-Classification/data/translated/english/english_test_tweets.csv\"\n",
    "e_test_df.to_csv(e_test_outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating Arabic Test Tweets...\n",
      "...tweet no. 200...\n",
      "...tweet no. 400...\n",
      "...tweet no. 600...\n",
      "...tweet no. 800...\n",
      "...tweet no. 1000...\n",
      "...tweet no. 1200...\n",
      "...tweet no. 1400...\n",
      "...tweet no. 1600...\n",
      "...tweet no. 1800...\n",
      "...tweet no. 2000...\n",
      "...tweet no. 2200...\n",
      "...tweet no. 2400...\n",
      "...tweet no. 2600...\n",
      "...tweet no. 2800...\n",
      "...tweet no. 3000...\n",
      "...tweet no. 3200...\n",
      "إجبار أبل على التعاون على فك شفرة اجهزتها\n",
      "To force Apple to cooperate in deciphering her devices.\n"
     ]
    }
   ],
   "source": [
    "# Translate arabic test tweets\n",
    "model_ar_en.to(device)\n",
    "a_test_transl = []\n",
    "print(\"Translating Arabic Test Tweets...\")\n",
    "for tweet in a_test_df['preprocessed']:\n",
    "    input_ids = tokenizer_ar_en.encode(tweet, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(device)\n",
    "    outputs = model_ar_en.generate(input_ids)\n",
    "    decoded = tokenizer_ar_en.decode(outputs[0], skip_special_tokens=True)\n",
    "    a_test_transl.append(decoded)\n",
    "    if len(a_test_transl) % 200 == 0:\n",
    "        print(\"...tweet no. \"+str(len(a_test_transl))+\"...\")\n",
    "\n",
    "print(a_test_df['preprocessed'][0])\n",
    "print(a_test_transl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save translated tweets\n",
    "a_test_df['translated'] = a_test_transl\n",
    "#print(a_test_df.head())\n",
    "\n",
    "a_test_outpath = \"/projectnb/cs505/reardonc/CS505-Spring-MultiLingual-Twitter-Classification/data/translated/arabic/arabic_test_tweets.csv\"\n",
    "a_test_df.to_csv(a_test_outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
