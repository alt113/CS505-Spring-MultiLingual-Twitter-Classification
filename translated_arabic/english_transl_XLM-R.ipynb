{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Fine-tuning & Testing XLM-Roberta on English-translated & English Data </h1>\n",
    "Carley Reardon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Train Tweets: (21124, 4)\n",
      "                   id sentiment  \\\n",
      "0  264183816548130816  positive   \n",
      "1  263405084770172928  negative   \n",
      "2  262163168678248449  negative   \n",
      "3  264249301910310912  negative   \n",
      "4  262682041215234048   neutral   \n",
      "\n",
      "                                            original  \\\n",
      "0  Gas by my house hit $3.39!!!! I\\u2019m going t...   \n",
      "1  Theo Walcott is still shit\\u002c watch Rafa an...   \n",
      "2  its not that I\\u2019m a GSP fan\\u002c i just h...   \n",
      "3  Iranian general says Israel\\u2019s Iron Dome c...   \n",
      "4  Tehran\\u002c Mon Amour: Obama Tried to Establi...   \n",
      "\n",
      "                                        preprocessed  \n",
      "0  gas house hit $ 3.39 i\\u2019 m going chapel hi...  \n",
      "1  theo walcott shit\\u002c watch rafa johnny deal...  \n",
      "2  i\\u2019 m gsp fan\\u002c hate nick diaz can\\u20...  \n",
      "3  iranian general says israel\\u2019s iron dome c...  \n",
      "4  tehran\\u002c mon amour obama tried establish t...  \n",
      "\n",
      "English Test Tweets: (28333, 4)\n",
      "                   id sentiment  \\\n",
      "0  264238274963451904  positive   \n",
      "1  218775148495515649  positive   \n",
      "2  258965201766998017   neutral   \n",
      "3  262926411352903682  negative   \n",
      "4  171874368908050432   neutral   \n",
      "\n",
      "                                            original  \\\n",
      "0  @jjuueellzz down in the Atlantic city, ventnor...   \n",
      "1  Musical awareness: Great Big Beautiful Tomorro...   \n",
      "2  On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...   \n",
      "3  Kapan sih lo ngebuktiin,jan ngomong doang Susa...   \n",
      "4  Excuse the connectivity of this live stream, f...   \n",
      "\n",
      "                                        preprocessed  \n",
      "0  atlantic city ventnor margate ocean city area ...  \n",
      "1  musical awareness great big beautiful tomorrow...  \n",
      "2  radio786 100.4fm 7:10 fri oct 19 labour analys...  \n",
      "3  kapan sih lo ngebuktiin jan ngomong doang susa...  \n",
      "4  excuse connectivity live stream baba amr activ...  \n",
      "\n",
      "Arabic-to-English Test Tweets: (3353, 6)\n",
      "   Unnamed: 0                  id sentiment  \\\n",
      "0           0  783555835494592513  positive   \n",
      "1           1  783582397166125056  positive   \n",
      "2           2  783592390728769536  positive   \n",
      "3           3  783597390070685696  positive   \n",
      "4           4  783617442031472640   neutral   \n",
      "\n",
      "                                            original  \\\n",
      "0  إجبار أبل على التعاون على فك شفرة اجهزتها http...   \n",
      "1  RT @20fourMedia: #غوغل تتحدى أبل وأمازون بأجهز...   \n",
      "2  جوجل تنافس أبل وسامسونج بهاتف جديد https://t.c...   \n",
      "3  رئيس شركة أبل: الواقع المعزز سيصبح أهم من الطع...   \n",
      "4  ساعة أبل في الأسواق مرة أخرى https://t.co/dY2x...   \n",
      "\n",
      "                                      preprocessed  \\\n",
      "0        إجبار أبل على التعاون على فك شفرة اجهزتها   \n",
      "1              غوغل تتحدى أبل وأمازون بأجهزة جديدة   \n",
      "2               جوجل تنافس أبل وسامسونج بهاتف جديد   \n",
      "3  رئيس شركة أبل الواقع المعزز سيصبح أهم من الطعام   \n",
      "4                     ساعة أبل في الأسواق مرة أخرى   \n",
      "\n",
      "                                          translated  \n",
      "0  To force Apple to cooperate in deciphering her...  \n",
      "1  Google is challenging Apple and Amazon with ne...  \n",
      "2  Google is competing with Apple and Samsung wit...  \n",
      "3  Apple's president, enhanced reality will becom...  \n",
      "4                  Apple hour's in the market again.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# first import our translated and original data\n",
    "e_prep_path = \"/projectnb/cs505/reardonc/CS505-Spring-MultiLingual-Twitter-Classification/data/preprocessed/english/\"\n",
    "a_transl_path = \"/projectnb/cs505/reardonc/CS505-Spring-MultiLingual-Twitter-Classification/data/translated/arabic/\"\n",
    "\n",
    "e_train_df = pd.read_csv(e_prep_path+\"english_train_tweets.csv\")\n",
    "e_test_df = pd.read_csv(e_prep_path+\"english_test_tweets.csv\")\n",
    "a_test_df = pd.read_csv(a_transl_path+\"arabic_test_tweets.csv\")\n",
    "\n",
    "print(\"English Train Tweets: \"+str(e_train_df.shape))\n",
    "print(e_train_df.head(5))\n",
    "print(\"\")\n",
    "print(\"English Test Tweets: \"+str(e_test_df.shape))\n",
    "print(e_test_df.head(5))\n",
    "print(\"\")\n",
    "print(\"Arabic-to-English Test Tweets: \"+str(a_test_df.shape))\n",
    "print(a_test_df.head(5))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (4.5.1)\n",
      "Requirement already satisfied: requests in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (1.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (4.46.0)\n",
      "Requirement already satisfied: sacremoses in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: filelock in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (2020.5.14)\n",
      "Requirement already satisfied: packaging in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: joblib in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: click in /share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /usr4/cs640/reardonc/.local/lib/python3.7/site-packages (0.1.95)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "# set up data for model\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=True)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "\n",
    "for idx, row in e_train_df.iterrows():\n",
    "    #print(row)\n",
    "    tw = str(row['preprocessed'])\n",
    "    if row['sentiment'] == \"positive\":\n",
    "        label = 1\n",
    "    elif row['sentiment'] == \"negative\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 2\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tw,                      \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 128,\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )   \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    labels.append(label)\n",
    "\n",
    "for idx, row in e_test_df.iterrows():\n",
    "    tw = str(row['preprocessed'])\n",
    "    if row['sentiment'] == \"positive\":\n",
    "        label = 1\n",
    "    elif row['sentiment'] == \"negative\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 2\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tw,                      \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 128,\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )   \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    labels.append(label)\n",
    "    \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44,511 training samples\n",
      "4,946 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "batch_size = 16\n",
    " \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "# Load XLM-R\n",
    "xlm_model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\", # Use the base XLM-R model, with an uncased vocab.\n",
    "    num_labels = 3, # The number of output labels--3 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "xlm_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# CODE BASED ON TUTORIAL BY CHRIS MCCORMICK AND NICK RYAN\n",
    "\n",
    "optimizer_xlm = AdamW(xlm_model.parameters(),\n",
    "                  lr = 5e-6,\n",
    "                  eps = 1e-8\n",
    "                )\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer_xlm, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def training_helper(model, train_dataloader, validation_dataloader, optimizer, scheduler, \n",
    "                    seed_val=42, epochs=3, ):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    training_stats = []\n",
    "\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            result = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels,\n",
    "                        return_dict=True)\n",
    "\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            with torch.no_grad():        \n",
    "                result = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels,\n",
    "                            return_dict=True)\n",
    "\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,782.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  2,782.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  2,782.    Elapsed: 0:00:17.\n",
      "  Batch   160  of  2,782.    Elapsed: 0:00:23.\n",
      "  Batch   200  of  2,782.    Elapsed: 0:00:29.\n",
      "  Batch   240  of  2,782.    Elapsed: 0:00:35.\n",
      "  Batch   280  of  2,782.    Elapsed: 0:00:41.\n",
      "  Batch   320  of  2,782.    Elapsed: 0:00:46.\n",
      "  Batch   360  of  2,782.    Elapsed: 0:00:52.\n",
      "  Batch   400  of  2,782.    Elapsed: 0:00:58.\n",
      "  Batch   440  of  2,782.    Elapsed: 0:01:04.\n",
      "  Batch   480  of  2,782.    Elapsed: 0:01:10.\n",
      "  Batch   520  of  2,782.    Elapsed: 0:01:16.\n",
      "  Batch   560  of  2,782.    Elapsed: 0:01:21.\n",
      "  Batch   600  of  2,782.    Elapsed: 0:01:27.\n",
      "  Batch   640  of  2,782.    Elapsed: 0:01:33.\n",
      "  Batch   680  of  2,782.    Elapsed: 0:01:39.\n",
      "  Batch   720  of  2,782.    Elapsed: 0:01:45.\n",
      "  Batch   760  of  2,782.    Elapsed: 0:01:51.\n",
      "  Batch   800  of  2,782.    Elapsed: 0:01:56.\n",
      "  Batch   840  of  2,782.    Elapsed: 0:02:02.\n",
      "  Batch   880  of  2,782.    Elapsed: 0:02:08.\n",
      "  Batch   920  of  2,782.    Elapsed: 0:02:14.\n",
      "  Batch   960  of  2,782.    Elapsed: 0:02:20.\n",
      "  Batch 1,000  of  2,782.    Elapsed: 0:02:25.\n",
      "  Batch 1,040  of  2,782.    Elapsed: 0:02:31.\n",
      "  Batch 1,080  of  2,782.    Elapsed: 0:02:37.\n",
      "  Batch 1,120  of  2,782.    Elapsed: 0:02:43.\n",
      "  Batch 1,160  of  2,782.    Elapsed: 0:02:49.\n",
      "  Batch 1,200  of  2,782.    Elapsed: 0:02:55.\n",
      "  Batch 1,240  of  2,782.    Elapsed: 0:03:00.\n",
      "  Batch 1,280  of  2,782.    Elapsed: 0:03:06.\n",
      "  Batch 1,320  of  2,782.    Elapsed: 0:03:12.\n",
      "  Batch 1,360  of  2,782.    Elapsed: 0:03:18.\n",
      "  Batch 1,400  of  2,782.    Elapsed: 0:03:24.\n",
      "  Batch 1,440  of  2,782.    Elapsed: 0:03:29.\n",
      "  Batch 1,480  of  2,782.    Elapsed: 0:03:35.\n",
      "  Batch 1,520  of  2,782.    Elapsed: 0:03:41.\n",
      "  Batch 1,560  of  2,782.    Elapsed: 0:03:47.\n",
      "  Batch 1,600  of  2,782.    Elapsed: 0:03:53.\n",
      "  Batch 1,640  of  2,782.    Elapsed: 0:03:58.\n",
      "  Batch 1,680  of  2,782.    Elapsed: 0:04:04.\n",
      "  Batch 1,720  of  2,782.    Elapsed: 0:04:10.\n",
      "  Batch 1,760  of  2,782.    Elapsed: 0:04:16.\n",
      "  Batch 1,800  of  2,782.    Elapsed: 0:04:22.\n",
      "  Batch 1,840  of  2,782.    Elapsed: 0:04:28.\n",
      "  Batch 1,880  of  2,782.    Elapsed: 0:04:33.\n",
      "  Batch 1,920  of  2,782.    Elapsed: 0:04:39.\n",
      "  Batch 1,960  of  2,782.    Elapsed: 0:04:45.\n",
      "  Batch 2,000  of  2,782.    Elapsed: 0:04:51.\n",
      "  Batch 2,040  of  2,782.    Elapsed: 0:04:57.\n",
      "  Batch 2,080  of  2,782.    Elapsed: 0:05:02.\n",
      "  Batch 2,120  of  2,782.    Elapsed: 0:05:08.\n",
      "  Batch 2,160  of  2,782.    Elapsed: 0:05:14.\n",
      "  Batch 2,200  of  2,782.    Elapsed: 0:05:20.\n",
      "  Batch 2,240  of  2,782.    Elapsed: 0:05:26.\n",
      "  Batch 2,280  of  2,782.    Elapsed: 0:05:31.\n",
      "  Batch 2,320  of  2,782.    Elapsed: 0:05:37.\n",
      "  Batch 2,360  of  2,782.    Elapsed: 0:05:43.\n",
      "  Batch 2,400  of  2,782.    Elapsed: 0:05:49.\n",
      "  Batch 2,440  of  2,782.    Elapsed: 0:05:55.\n",
      "  Batch 2,480  of  2,782.    Elapsed: 0:06:01.\n",
      "  Batch 2,520  of  2,782.    Elapsed: 0:06:06.\n",
      "  Batch 2,560  of  2,782.    Elapsed: 0:06:12.\n",
      "  Batch 2,600  of  2,782.    Elapsed: 0:06:18.\n",
      "  Batch 2,640  of  2,782.    Elapsed: 0:06:24.\n",
      "  Batch 2,680  of  2,782.    Elapsed: 0:06:30.\n",
      "  Batch 2,720  of  2,782.    Elapsed: 0:06:35.\n",
      "  Batch 2,760  of  2,782.    Elapsed: 0:06:41.\n",
      "\n",
      "  Average training loss: 0.84\n",
      "  Training epcoh took: 0:06:44\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.66\n",
      "  Validation Loss: 0.76\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,782.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  2,782.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  2,782.    Elapsed: 0:00:17.\n",
      "  Batch   160  of  2,782.    Elapsed: 0:00:23.\n",
      "  Batch   200  of  2,782.    Elapsed: 0:00:29.\n",
      "  Batch   240  of  2,782.    Elapsed: 0:00:35.\n",
      "  Batch   280  of  2,782.    Elapsed: 0:00:41.\n",
      "  Batch   320  of  2,782.    Elapsed: 0:00:46.\n",
      "  Batch   360  of  2,782.    Elapsed: 0:00:52.\n",
      "  Batch   400  of  2,782.    Elapsed: 0:00:58.\n",
      "  Batch   440  of  2,782.    Elapsed: 0:01:04.\n",
      "  Batch   480  of  2,782.    Elapsed: 0:01:10.\n",
      "  Batch   520  of  2,782.    Elapsed: 0:01:16.\n",
      "  Batch   560  of  2,782.    Elapsed: 0:01:21.\n",
      "  Batch   600  of  2,782.    Elapsed: 0:01:27.\n",
      "  Batch   640  of  2,782.    Elapsed: 0:01:33.\n",
      "  Batch   680  of  2,782.    Elapsed: 0:01:39.\n",
      "  Batch   720  of  2,782.    Elapsed: 0:01:45.\n",
      "  Batch   760  of  2,782.    Elapsed: 0:01:50.\n",
      "  Batch   800  of  2,782.    Elapsed: 0:01:56.\n",
      "  Batch   840  of  2,782.    Elapsed: 0:02:02.\n",
      "  Batch   880  of  2,782.    Elapsed: 0:02:08.\n",
      "  Batch   920  of  2,782.    Elapsed: 0:02:14.\n",
      "  Batch   960  of  2,782.    Elapsed: 0:02:19.\n",
      "  Batch 1,000  of  2,782.    Elapsed: 0:02:25.\n",
      "  Batch 1,040  of  2,782.    Elapsed: 0:02:31.\n",
      "  Batch 1,080  of  2,782.    Elapsed: 0:02:37.\n",
      "  Batch 1,120  of  2,782.    Elapsed: 0:02:43.\n",
      "  Batch 1,160  of  2,782.    Elapsed: 0:02:48.\n",
      "  Batch 1,200  of  2,782.    Elapsed: 0:02:54.\n",
      "  Batch 1,240  of  2,782.    Elapsed: 0:03:00.\n",
      "  Batch 1,280  of  2,782.    Elapsed: 0:03:06.\n",
      "  Batch 1,320  of  2,782.    Elapsed: 0:03:12.\n",
      "  Batch 1,360  of  2,782.    Elapsed: 0:03:17.\n",
      "  Batch 1,400  of  2,782.    Elapsed: 0:03:23.\n",
      "  Batch 1,440  of  2,782.    Elapsed: 0:03:29.\n",
      "  Batch 1,480  of  2,782.    Elapsed: 0:03:35.\n",
      "  Batch 1,520  of  2,782.    Elapsed: 0:03:41.\n",
      "  Batch 1,560  of  2,782.    Elapsed: 0:03:47.\n",
      "  Batch 1,600  of  2,782.    Elapsed: 0:03:53.\n",
      "  Batch 1,640  of  2,782.    Elapsed: 0:03:59.\n",
      "  Batch 1,680  of  2,782.    Elapsed: 0:04:04.\n",
      "  Batch 1,720  of  2,782.    Elapsed: 0:04:10.\n",
      "  Batch 1,760  of  2,782.    Elapsed: 0:04:16.\n",
      "  Batch 1,800  of  2,782.    Elapsed: 0:04:22.\n",
      "  Batch 1,840  of  2,782.    Elapsed: 0:04:28.\n",
      "  Batch 1,880  of  2,782.    Elapsed: 0:04:34.\n",
      "  Batch 1,920  of  2,782.    Elapsed: 0:04:40.\n",
      "  Batch 1,960  of  2,782.    Elapsed: 0:04:46.\n",
      "  Batch 2,000  of  2,782.    Elapsed: 0:04:51.\n",
      "  Batch 2,040  of  2,782.    Elapsed: 0:04:57.\n",
      "  Batch 2,080  of  2,782.    Elapsed: 0:05:03.\n",
      "  Batch 2,120  of  2,782.    Elapsed: 0:05:09.\n",
      "  Batch 2,160  of  2,782.    Elapsed: 0:05:15.\n",
      "  Batch 2,200  of  2,782.    Elapsed: 0:05:21.\n",
      "  Batch 2,240  of  2,782.    Elapsed: 0:05:27.\n",
      "  Batch 2,280  of  2,782.    Elapsed: 0:05:33.\n",
      "  Batch 2,320  of  2,782.    Elapsed: 0:05:38.\n",
      "  Batch 2,360  of  2,782.    Elapsed: 0:05:44.\n",
      "  Batch 2,400  of  2,782.    Elapsed: 0:05:50.\n",
      "  Batch 2,440  of  2,782.    Elapsed: 0:05:56.\n",
      "  Batch 2,480  of  2,782.    Elapsed: 0:06:02.\n",
      "  Batch 2,520  of  2,782.    Elapsed: 0:06:08.\n",
      "  Batch 2,560  of  2,782.    Elapsed: 0:06:14.\n",
      "  Batch 2,600  of  2,782.    Elapsed: 0:06:20.\n",
      "  Batch 2,640  of  2,782.    Elapsed: 0:06:25.\n",
      "  Batch 2,680  of  2,782.    Elapsed: 0:06:31.\n",
      "  Batch 2,720  of  2,782.    Elapsed: 0:06:37.\n",
      "  Batch 2,760  of  2,782.    Elapsed: 0:06:43.\n",
      "\n",
      "  Average training loss: 0.74\n",
      "  Training epcoh took: 0:06:46\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.67\n",
      "  Validation Loss: 0.73\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,782.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  2,782.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  2,782.    Elapsed: 0:00:17.\n",
      "  Batch   160  of  2,782.    Elapsed: 0:00:23.\n",
      "  Batch   200  of  2,782.    Elapsed: 0:00:29.\n",
      "  Batch   240  of  2,782.    Elapsed: 0:00:35.\n",
      "  Batch   280  of  2,782.    Elapsed: 0:00:41.\n",
      "  Batch   320  of  2,782.    Elapsed: 0:00:46.\n",
      "  Batch   360  of  2,782.    Elapsed: 0:00:52.\n",
      "  Batch   400  of  2,782.    Elapsed: 0:00:58.\n",
      "  Batch   440  of  2,782.    Elapsed: 0:01:04.\n",
      "  Batch   480  of  2,782.    Elapsed: 0:01:10.\n",
      "  Batch   520  of  2,782.    Elapsed: 0:01:15.\n",
      "  Batch   560  of  2,782.    Elapsed: 0:01:21.\n",
      "  Batch   600  of  2,782.    Elapsed: 0:01:27.\n",
      "  Batch   640  of  2,782.    Elapsed: 0:01:33.\n",
      "  Batch   680  of  2,782.    Elapsed: 0:01:39.\n",
      "  Batch   720  of  2,782.    Elapsed: 0:01:45.\n",
      "  Batch   760  of  2,782.    Elapsed: 0:01:50.\n",
      "  Batch   800  of  2,782.    Elapsed: 0:01:56.\n",
      "  Batch   840  of  2,782.    Elapsed: 0:02:02.\n",
      "  Batch   880  of  2,782.    Elapsed: 0:02:08.\n",
      "  Batch   920  of  2,782.    Elapsed: 0:02:14.\n",
      "  Batch   960  of  2,782.    Elapsed: 0:02:19.\n",
      "  Batch 1,000  of  2,782.    Elapsed: 0:02:25.\n",
      "  Batch 1,040  of  2,782.    Elapsed: 0:02:31.\n",
      "  Batch 1,080  of  2,782.    Elapsed: 0:02:37.\n",
      "  Batch 1,120  of  2,782.    Elapsed: 0:02:43.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,160  of  2,782.    Elapsed: 0:02:48.\n",
      "  Batch 1,200  of  2,782.    Elapsed: 0:02:54.\n",
      "  Batch 1,240  of  2,782.    Elapsed: 0:03:00.\n",
      "  Batch 1,280  of  2,782.    Elapsed: 0:03:06.\n",
      "  Batch 1,320  of  2,782.    Elapsed: 0:03:12.\n",
      "  Batch 1,360  of  2,782.    Elapsed: 0:03:17.\n",
      "  Batch 1,400  of  2,782.    Elapsed: 0:03:23.\n",
      "  Batch 1,440  of  2,782.    Elapsed: 0:03:29.\n",
      "  Batch 1,480  of  2,782.    Elapsed: 0:03:35.\n",
      "  Batch 1,520  of  2,782.    Elapsed: 0:03:41.\n",
      "  Batch 1,560  of  2,782.    Elapsed: 0:03:46.\n",
      "  Batch 1,600  of  2,782.    Elapsed: 0:03:52.\n",
      "  Batch 1,640  of  2,782.    Elapsed: 0:03:58.\n",
      "  Batch 1,680  of  2,782.    Elapsed: 0:04:04.\n",
      "  Batch 1,720  of  2,782.    Elapsed: 0:04:10.\n",
      "  Batch 1,760  of  2,782.    Elapsed: 0:04:15.\n",
      "  Batch 1,800  of  2,782.    Elapsed: 0:04:21.\n",
      "  Batch 1,840  of  2,782.    Elapsed: 0:04:27.\n",
      "  Batch 1,880  of  2,782.    Elapsed: 0:04:33.\n",
      "  Batch 1,920  of  2,782.    Elapsed: 0:04:39.\n",
      "  Batch 1,960  of  2,782.    Elapsed: 0:04:44.\n",
      "  Batch 2,000  of  2,782.    Elapsed: 0:04:50.\n",
      "  Batch 2,040  of  2,782.    Elapsed: 0:04:56.\n",
      "  Batch 2,080  of  2,782.    Elapsed: 0:05:02.\n",
      "  Batch 2,120  of  2,782.    Elapsed: 0:05:08.\n",
      "  Batch 2,160  of  2,782.    Elapsed: 0:05:13.\n",
      "  Batch 2,200  of  2,782.    Elapsed: 0:05:19.\n",
      "  Batch 2,240  of  2,782.    Elapsed: 0:05:25.\n",
      "  Batch 2,280  of  2,782.    Elapsed: 0:05:31.\n",
      "  Batch 2,320  of  2,782.    Elapsed: 0:05:37.\n",
      "  Batch 2,360  of  2,782.    Elapsed: 0:05:42.\n",
      "  Batch 2,400  of  2,782.    Elapsed: 0:05:48.\n",
      "  Batch 2,440  of  2,782.    Elapsed: 0:05:54.\n",
      "  Batch 2,480  of  2,782.    Elapsed: 0:06:00.\n",
      "  Batch 2,520  of  2,782.    Elapsed: 0:06:06.\n",
      "  Batch 2,560  of  2,782.    Elapsed: 0:06:11.\n",
      "  Batch 2,600  of  2,782.    Elapsed: 0:06:17.\n",
      "  Batch 2,640  of  2,782.    Elapsed: 0:06:23.\n",
      "  Batch 2,680  of  2,782.    Elapsed: 0:06:29.\n",
      "  Batch 2,720  of  2,782.    Elapsed: 0:06:35.\n",
      "  Batch 2,760  of  2,782.    Elapsed: 0:06:40.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epcoh took: 0:06:44\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.68\n",
      "  Validation Loss: 0.74\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,782.    Elapsed: 0:00:06.\n",
      "  Batch    80  of  2,782.    Elapsed: 0:00:12.\n",
      "  Batch   120  of  2,782.    Elapsed: 0:00:17.\n",
      "  Batch   160  of  2,782.    Elapsed: 0:00:23.\n",
      "  Batch   200  of  2,782.    Elapsed: 0:00:29.\n",
      "  Batch   240  of  2,782.    Elapsed: 0:00:35.\n",
      "  Batch   280  of  2,782.    Elapsed: 0:00:41.\n",
      "  Batch   320  of  2,782.    Elapsed: 0:00:46.\n",
      "  Batch   360  of  2,782.    Elapsed: 0:00:52.\n",
      "  Batch   400  of  2,782.    Elapsed: 0:00:58.\n",
      "  Batch   440  of  2,782.    Elapsed: 0:01:04.\n",
      "  Batch   480  of  2,782.    Elapsed: 0:01:10.\n",
      "  Batch   520  of  2,782.    Elapsed: 0:01:15.\n",
      "  Batch   560  of  2,782.    Elapsed: 0:01:21.\n",
      "  Batch   600  of  2,782.    Elapsed: 0:01:27.\n",
      "  Batch   640  of  2,782.    Elapsed: 0:01:33.\n",
      "  Batch   680  of  2,782.    Elapsed: 0:01:39.\n",
      "  Batch   720  of  2,782.    Elapsed: 0:01:44.\n",
      "  Batch   760  of  2,782.    Elapsed: 0:01:50.\n",
      "  Batch   800  of  2,782.    Elapsed: 0:01:56.\n",
      "  Batch   840  of  2,782.    Elapsed: 0:02:02.\n",
      "  Batch   880  of  2,782.    Elapsed: 0:02:08.\n",
      "  Batch   920  of  2,782.    Elapsed: 0:02:13.\n",
      "  Batch   960  of  2,782.    Elapsed: 0:02:19.\n",
      "  Batch 1,000  of  2,782.    Elapsed: 0:02:25.\n",
      "  Batch 1,040  of  2,782.    Elapsed: 0:02:31.\n",
      "  Batch 1,080  of  2,782.    Elapsed: 0:02:37.\n",
      "  Batch 1,120  of  2,782.    Elapsed: 0:02:43.\n",
      "  Batch 1,160  of  2,782.    Elapsed: 0:02:48.\n",
      "  Batch 1,200  of  2,782.    Elapsed: 0:02:54.\n",
      "  Batch 1,240  of  2,782.    Elapsed: 0:03:00.\n",
      "  Batch 1,280  of  2,782.    Elapsed: 0:03:06.\n",
      "  Batch 1,320  of  2,782.    Elapsed: 0:03:12.\n",
      "  Batch 1,360  of  2,782.    Elapsed: 0:03:17.\n",
      "  Batch 1,400  of  2,782.    Elapsed: 0:03:23.\n",
      "  Batch 1,440  of  2,782.    Elapsed: 0:03:29.\n",
      "  Batch 1,480  of  2,782.    Elapsed: 0:03:35.\n",
      "  Batch 1,520  of  2,782.    Elapsed: 0:03:41.\n",
      "  Batch 1,560  of  2,782.    Elapsed: 0:03:46.\n",
      "  Batch 1,600  of  2,782.    Elapsed: 0:03:52.\n",
      "  Batch 1,640  of  2,782.    Elapsed: 0:03:58.\n",
      "  Batch 1,680  of  2,782.    Elapsed: 0:04:04.\n",
      "  Batch 1,720  of  2,782.    Elapsed: 0:04:10.\n",
      "  Batch 1,760  of  2,782.    Elapsed: 0:04:15.\n",
      "  Batch 1,800  of  2,782.    Elapsed: 0:04:21.\n",
      "  Batch 1,840  of  2,782.    Elapsed: 0:04:27.\n",
      "  Batch 1,880  of  2,782.    Elapsed: 0:04:33.\n",
      "  Batch 1,920  of  2,782.    Elapsed: 0:04:39.\n",
      "  Batch 1,960  of  2,782.    Elapsed: 0:04:44.\n",
      "  Batch 2,000  of  2,782.    Elapsed: 0:04:50.\n",
      "  Batch 2,040  of  2,782.    Elapsed: 0:04:56.\n",
      "  Batch 2,080  of  2,782.    Elapsed: 0:05:02.\n",
      "  Batch 2,120  of  2,782.    Elapsed: 0:05:08.\n",
      "  Batch 2,160  of  2,782.    Elapsed: 0:05:13.\n",
      "  Batch 2,200  of  2,782.    Elapsed: 0:05:19.\n",
      "  Batch 2,240  of  2,782.    Elapsed: 0:05:25.\n",
      "  Batch 2,280  of  2,782.    Elapsed: 0:05:31.\n",
      "  Batch 2,320  of  2,782.    Elapsed: 0:05:37.\n",
      "  Batch 2,360  of  2,782.    Elapsed: 0:05:42.\n",
      "  Batch 2,400  of  2,782.    Elapsed: 0:05:48.\n",
      "  Batch 2,440  of  2,782.    Elapsed: 0:05:54.\n",
      "  Batch 2,480  of  2,782.    Elapsed: 0:06:00.\n",
      "  Batch 2,520  of  2,782.    Elapsed: 0:06:06.\n",
      "  Batch 2,560  of  2,782.    Elapsed: 0:06:11.\n",
      "  Batch 2,600  of  2,782.    Elapsed: 0:06:17.\n",
      "  Batch 2,640  of  2,782.    Elapsed: 0:06:23.\n",
      "  Batch 2,680  of  2,782.    Elapsed: 0:06:29.\n",
      "  Batch 2,720  of  2,782.    Elapsed: 0:06:35.\n",
      "  Batch 2,760  of  2,782.    Elapsed: 0:06:40.\n",
      "\n",
      "  Average training loss: 0.67\n",
      "  Training epcoh took: 0:06:44\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.68\n",
      "  Validation Loss: 0.73\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:27:42 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "xlm_training_stats = training_helper(xlm_model, train_dataloader, val_dataloader, optimizer_xlm, scheduler, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/pytorch/1.5/install/3.7/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# save model\n",
    "pickle.dump(xlm_model, open(\"./xlm_english.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0:06:44</td>\n",
       "      <td>0:00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0:06:46</td>\n",
       "      <td>0:00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0:06:44</td>\n",
       "      <td>0:00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0:06:44</td>\n",
       "      <td>0:00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.84         0.76           0.66       0:06:44         0:00:11\n",
       "2               0.74         0.73           0.67       0:06:46         0:00:11\n",
       "3               0.69         0.74           0.68       0:06:44         0:00:11\n",
       "4               0.67         0.73           0.68       0:06:44         0:00:11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show training stats\n",
    "pd.set_option('precision', 2)\n",
    "df_stats = pd.DataFrame(data=xlm_training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr4/cs640/reardonc/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# test on arabic tweets\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "test_labels = []\n",
    "\n",
    "# For every sentence...\n",
    "for idx, row in a_test_df.iterrows():\n",
    "    tw = str(row['translated'])\n",
    "    if row['sentiment'] == \"positive\":\n",
    "        label = 1\n",
    "    elif row['sentiment'] == \"negative\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 2\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tw,                      \n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )    \n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "    test_labels.append(label)\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 16  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 3,353 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "xlm_model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        result = xlm_model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           return_dict=True)\n",
    "        \n",
    "    logits = result.logits\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy:  0.5965608465608465\n"
     ]
    }
   ],
   "source": [
    "xlm_accuracies = []\n",
    "for i in range(len(predictions)):\n",
    "    xlm_accuracies += [flat_accuracy(predictions[i], true_labels[i])]\n",
    "\n",
    "xlm_mean = sum(xlm_accuracies)/len(xlm_accuracies)\n",
    "#print('Accuracies for each test batch: ', xlm_accuracies)\n",
    "print('Average test accuracy: ', xlm_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
